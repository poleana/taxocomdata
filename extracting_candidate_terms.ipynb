{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import spacy\n",
    "import copy\n",
    "import nltk\n",
    "from nltk.corpus import mac_morpho\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('mac_morpho')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('treebank')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unidecode\n",
    "!python -m spacy download pt_core_news_lg\n",
    "!pip install git+https://github.com/LIAAD/yake\n",
    "!pip install git+https://github.com/LIAAD/yake --upgrade\n",
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import yake\n",
    "import textacy\n",
    "NLP = spacy.load(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Itau-Unibanco/FAQ_BACEN\")\n",
    "train_dataset = load_dataset(\"Itau-Unibanco/FAQ_BACEN\", split=\"train\")\n",
    "train_dataset['categories']\n",
    "{'Unnamed: 0': Value(dtype='int64', id=None),\n",
    " 'questions': Value(dtype='string', id=None),\n",
    " 'categories': Value(dtype='string', id=None),\n",
    " 'answers': Value(dtype='string', id=None)}\n",
    "\n",
    "categorias = train_dataset['categories']\n",
    "questions =  train_dataset['questions']\n",
    "answers =  train_dataset['answers']\n",
    "\n",
    "all_data = categorias\n",
    "for q in questions:\n",
    "  all_data.append(q)\n",
    "for q in answers:\n",
    "  all_data.append(q)\n",
    "\n",
    "df_data = {'descricao': all_data}\n",
    "data_shopping = pd.DataFrame(df_data)\n",
    "\n",
    "generic_words = ['de',  'a',  'o',  'que',  'e',  'do',  'da',  'em',  'um',  'para',  'é',  'com',  'não',  'uma',  'os',  'no',  'se',  'na',  'por',  'mais',  'as',  'dos',  'como',  'mas',  'foi',  'ao',  'ele',  'das',  'tem',  'à',  'seu',  'sua',  'ou',  'ser',  'quando',  'muito',  'há',  'nos',  'já',  'está',  'eu',  'também',  'só',  'pelo',  'pela',  'até',  'isso',  'ela',  'entre',  'era',  'depois',  'sem',  'mesmo',  'aos',  'ter',  'seus',  'quem',  'nas',  'me',  'esse',  'eles',  'estão',  'você',  'tinha',  'foram',  'essa',  'num',  'nem',  'suas',  'meu',  'às',  'minha',  'têm',  'numa',  'pelos',  'elas',  'havia',  'seja',  'qual',  'será',  'nós',  'tenho',  'lhe',  'deles',  'essas',  'esses',  'pelas',  'este',  'fosse',  'dele',  'tu',  'te',  'vocês',  'vos',  'lhes',  'meus',  'minhas',  'teu',  'tua',  'teus',  'tuas',  'nosso',  'nossa',  'nossos',  'nossas',  'dela',  'delas',  'esta',  'estes',  'estas',  'aquele',  'aquela',  'aqueles',  'aquelas',  'isto',  'aquilo',  'estou',  'está',  'estamos',  'estão',  'estive',  'esteve',  'estivemos',  'estiveram',  'estava',  'estávamos',  'estavam',  'estivera',  'estivéramos',  'esteja',  'estejamos',  'estejam',  'estivesse',  'estivéssemos',  'estivessem',  'estiver',  'estivermos',  'estiverem',  'hei',  'há',  'havemos',  'hão',  'houve',  'houvemos',  'houveram',  'houvera',  'houvéramos',  'haja',  'hajamos',  'hajam',  'houvesse',  'houvéssemos',  'houvessem',  'houver',  'houvermos',  'houverem',  'houverei',  'houverá',  'houveremos',  'houverão',  'houveria',  'houveríamos',  'houveriam',  'sou',  'somos',  'são',  'era',  'éramos',  'eram',  'fui',  'foi',  'fomos',  'foram',  'fora',  'fôramos',  'seja',  'sejamos',  'sejam',  'fosse',  'fôssemos',  'fossem',  'for',  'formos',  'forem',  'serei',  'será',  'seremos',  'serão',  'seria',  'seríamos',  'seriam',  'tenho',  'tem',  'temos',  'tém',  'tinha',  'tínhamos',  'tinham',  'tive',  'teve',  'tivemos',  'tiveram',  'tivera',  'tivéramos',  'tenha',  'tenhamos',  'tenham',  'tivesse',  'tivéssemos',  'tivessem',  'tiver',  'tivermos',  'tiverem',  'terei',  'terá',  'teremos',  'terão',  'teria',  'teríamos',  'teriam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descs_ngrams = []\n",
    "\n",
    "def filter_preprocess(filter='generic'):\n",
    "    all_descs_bigrams = []\n",
    "    sentences = list(data_shopping['descricao'].values)\n",
    "    stop_words = nltk.corpus.stopwords.words('portuguese')\n",
    "    filtered_sentences=[]\n",
    "\n",
    "    #initiate lemmas with the generic_words lemmas\n",
    "    gen_words_lemmas = []\n",
    "    for word in generic_words:\n",
    "        generic_tagger = NLP(word)\n",
    "        for token in generic_tagger:\n",
    "            if token.lemma_ not in gen_words_lemmas:\n",
    "                gen_words_lemmas.append(token.lemma_)\n",
    "\n",
    "    #preprocess each sentence description\n",
    "    for sentence in tqdm(sentences):\n",
    "        text = sentence\n",
    "        text=unidecode.unidecode(str(text).lower())\n",
    "        text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "        sentence = (\" \".join(text.split()))\n",
    "        #print(sentence +\"\\n\")\n",
    "\n",
    "        words = []\n",
    "        chunks =[]\n",
    "        s_bigrams = []\n",
    "        lemmas = []\n",
    "        lemmas.extend(gen_words_lemmas)\n",
    "        if len(sentence) > 2:\n",
    "            tagger = NLP(sentence)\n",
    "            # looking for bigrams and trigrams in the sentence\n",
    "            # chunks = list(textacy.extract.ngrams(tagger, 2, min_freq=6))\n",
    "            # chunks.extend(list(textacy.extract.ngrams(tagger, 3, min_freq=5)))\n",
    "            # for word in chunks:\n",
    "            #     word = str(word).replace(\" \", \"_\")\n",
    "            #     #only save the ngram if it is in the reference list\n",
    "            #     if (word not in s_bigrams):# and (word in bigramas_ref):\n",
    "            #         s_bigrams.append(word)\n",
    "            #print(s_bigrams)\n",
    "\n",
    "            #filtering some gramatical classes, generic and stop_words\n",
    "            for token in tagger:\n",
    "                if token.pos_ not in ['ADV', 'CCONJ', 'ADP', 'AUX', 'CONJ', 'DET', 'INTJ', 'PART', 'PRON', 'PUNCT', 'SYM','SCONJ','ADJ','VERB', 'PROPN']:\n",
    "                    text = str(token.orth_)\n",
    "                    #print(text, token.lemma_)\n",
    "                    if text not in stop_words and text not in generic_words:\n",
    "                        if token.lemma_ not in lemmas:\n",
    "                            words.append(text)\n",
    "                            lemmas.append(token.lemma_)\n",
    "\n",
    "            #print(lemmas)\n",
    "            #words.extend(s_bigrams)\n",
    "            sentence= ' '.join(words)\n",
    "            #print(sentence)\n",
    "            #break\n",
    "        #all_descs_ngrams.append(s_bigrams)\n",
    "        filtered_sentences.append(sentence)\n",
    "\n",
    "    return filtered_sentences\n",
    "\n",
    "#[ADV, CCONJ, ADP, AUX, CONJ, DET, INTJ, PART, PRON, PUNCT, SYM,SCONJ,ADJ,VERB, PROPN]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se atualizada a lista de palavras genericas, rodar a partir desse ponto\n",
    "filtered_sentences = []\n",
    "\n",
    "filtered_sentences = filter_preprocess()\n",
    "\n",
    "#data_shopping['descricao_processed'] = filtered_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de palavras chaves com o YAKE!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_semelhantes(words_list):\n",
    "    palavras_finais = []\n",
    "    for word in words_list:\n",
    "        if word not in palavras_finais:\n",
    "            palavras_finais.append(word)\n",
    "    #print(palavras_finais)\n",
    "    words_list =palavras_finais.copy()\n",
    "    for t in range(0, len(words_list)-1):\n",
    "        for i in range(t+1, len(words_list)):\n",
    "            maxW = max(len(words_list[t]), len(words_list[i]))\n",
    "            distance=nltk.edit_distance(words_list[t] , words_list[i])\n",
    "            #print((1- distance/maxW))\n",
    "            if (1- distance/maxW) > 0.68:\n",
    "                maxP = max(words_list[t], words_list[i])\n",
    "                if maxP in palavras_finais:\n",
    "                    palavras_finais.remove(maxP)\n",
    "    return palavras_finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_nodes = {}\n",
    "nodes = ['bacen']\n",
    "tipos = ['bacen'] * len(filtered_sentences)\n",
    "df_yake = {'tipo':tipos,'descricao_processed':filtered_sentences}\n",
    "data_shopping = pd.DataFrame(df_yake) \n",
    "data_shopping.dropna(subset=['descricao_processed']).reset_index(drop=True)\n",
    "\n",
    "language = \"pt\"\n",
    "max_ngram_size = 1\n",
    "deduplication_threshold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 40\n",
    "\n",
    "for node in tqdm(nodes):\n",
    "    print(\"\\n\",node)\n",
    "    data_node = data_shopping[data_shopping['tipo'] == node]\n",
    "\n",
    "    indexes = list(data_node.index)\n",
    "    #print(max(indexes))\n",
    "    # ngrams_node = []\n",
    "    # for i in indexes:\n",
    "    #     ngrams_node.extend(all_descs_ngrams[i])\n",
    "\n",
    "    # # #print(node+ \":\", ngrams_node)\n",
    "    # ngrams_node = remove_semelhantes(ngrams_node)\n",
    "    #print(node+ \":\", ngrams_node)\n",
    "\n",
    "    sentences=list(data_node['descricao_processed'].values)\n",
    "\n",
    "    #Processando as palavras-chaves\n",
    "    text = ' '.join(map(str, sentences))\n",
    "\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    words = []\n",
    "    #values = []\n",
    "    for kw in keywords:\n",
    "        #values.append(kw[1])\n",
    "        words.append(kw[0])\n",
    "\n",
    "    palavras_finais = []\n",
    "    palavras_finais = remove_semelhantes(words)\n",
    "\n",
    "    #palavras_finais.extend(ngrams_node)\n",
    "    dict_nodes[node] = palavras_finais\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for node in nodes:\n",
    "    all_words.extend(dict_nodes[node])\n",
    "    print(node, len(dict_nodes[node]))\n",
    "    print(dict_nodes[node], '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
